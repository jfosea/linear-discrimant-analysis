\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setsansfont[]{Calibri Light}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage[margin=2.54cm]{geometry}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\author{}
\date{\vspace{-2.5em}}

\begin{document}

\fontsize{11}{15} \selectfont

\begin{titlepage}
  \vspace*{\fill}
  \begin{center}
    \LARGE{\textbf{Creating Classification Rules to Distinguish Between Cherry Tree and Pear Tree Leaves}}\\[3.5 cm]
    \large{Presented to Dr. Steven Vamosi}\\[0.5cm]
    \large{Professor of Ecology and Evolutionary Biology} \\[0.5cm]
    \large{University of Calgary} \\[3.5cm]
    \large{Prepared by Jana Osea} \\[0.5cm]
    \large{Undergraduate Student in Statistics}\\[0.5cm]
    \large{University of Calgary}\\[3.5cm]
    \large{March 14, 2021}
  \end{center}
  \vspace*{\fill}
\end{titlepage}

\tableofcontents

\newpage

\section{{Summary}}

\newpage

\section{{Introduction}}

In many fields of study, classifying items or individuals as belonging
to one of two or more population or groups is an integral part of
analysis. In many cases, it is often the goal of a research or study to
classify each sample item correctly. In fields like finance and medical
research, correctly classifying items can imply stopping transaction
fraud or discovering deadly tumors. Hence, it is important to understand
how classification procedures work.

\subsection{\normalsize{\textit{Background}}}

As the importance of classification is becoming more evident, Professor
Steven Vamosi, a Doctor in Ecology and Evolutionary Biology has
entrusted an undergraduate student in Statistics at the University of
Calgary, Jana Osea, with the task to develop a classification rule to
distinguish between cherry and pear tree leaves. This allows Dr.~Vamosi
a simple method to classify between the two species and for us
demonstrate our knowledge of classification.

\subsection{\normalsize{\textit{Goal}}}

Using width and length measurements taken from cherry and pear tree
leaves, our \textit{goal} is to create a classification method to
distinguish between the two species.

\newpage

\section{Data Generation Process}

\subsection{\normalsize{\textit{Data Source}}}

\begin{itemize}
\tightlist
\item
  Cherry leaves: Figure 7.2 of the pdf file printed on a standard A4
  paper
\item
  Pear leaves: Figure 7.3 of the pdf file printed on a standard A4 paper
\end{itemize}

\subsection{\normalsize{\textit{Data Input}}}

In an Microsoft Excel Sheet (2020), I prepared 3 empty columns with the
following headers: species, width, and length.

For each leaf a new row with 3 columns is recorded in the excel sheet
that contains the species, width, and length measured according to the
procedure outlined below. After recording each value, I saved the data
as a csv file named ``data.csv''. In addition, the full raw data can be
found in A1 in the appendix.

\begin{itemize}
\tightlist
\item
  species: If the leaf is part of figure 7.2, then species contains
  string input ``cherry.'' If the leaf is part of figure 7.3, then
  species contains string input ``pear.''
\item
  width: Measured the widest part of each leaf using a straight ruler to
  the closest millimeter
\item
  length: Measured from the bottom tip to the top tip using a straight
  ruler of each leaf to the closest millimeter
\end{itemize}

\subsection{\normalsize{\textit{Methods}}}

\textit{Overview of Methods}

After inputting the entire data set, I imported the csv file into my
program. I made 2 classifications: (1) with equal variance assumption
and (2) with no equal variance assumption. Densities and lambda values
were calculated for each leaf and visualizations of classifcations were
made. 3 new leaf measurements were provided and classified according to
the first classification. In addition, misclassifications of each method
were recorded.

\textit{Software and Packages}

I used R version 4.0.3 (2020-10-10) (R Core Team (2020)) to perform all
my classification programming. I also used the following R package to
help me visualize and aid my density calculations

\begin{itemize}
\tightlist
\item
  ggplot2 (H. Wickham (2016))
\item
  gridarrange (Baptiste Auguie (2017))
\item
  mtvnorm (Alan Genz, et. al (2020))
\end{itemize}

\newpage

\section{First Classification}

\subsection{\normalsize{\textit{Assumptions}}}

The first classification rule assumes the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For each species \(k = \text{cherry or pear}\), the distribution of
  the width and length measurements follow a bivariate normal
  distribution as follows

  \[\left(\begin{matrix}X_k \\ Y_k \end{matrix} \right) \sim N_2 \left(\mu_{k}, \Sigma \right)\]
  where

  \begin{align*}
  X_k &= \text{ width (mm) of the $k$ species} \\
  Y_k &= \text{ length (mm) of the $k$ species} \\
  \mu_k &= \left( \begin{matrix}\mu_{kx} \\ \mu_{ky} \end{matrix}\right) \text{ of the $k$ species} \\
  \Sigma &= \left(\begin{matrix} \sigma^2_x &\sigma_{xy} \\ \sigma_{xy} &\sigma^2_{y} \end{matrix} \right)
  \end{align*}

  Hence, the density of a leaf given the \(x\) width and \(y\) length
  according to the \(k = \text{cherry or pear}\) species is given by

  \[f_k(x,y \mid \mu_k, \Sigma) = \frac{1}{2\pi\sqrt{|\Sigma|}} \exp\left[\frac{-1}{2}\ \left(\begin{matrix}x-\mu_{kx} \\ y-\mu_{ky} \end{matrix}\right)^T \Sigma^{-1} \left(\begin{matrix}x-\mu_{kx} \\ y-\mu_{ky} \end{matrix}\right) \right] \]

  where \[|\Sigma| = \text{ determinant of the covariance matrix}.\]
\item
  The covariance matrix \(\Sigma\) of the \(k = \text{cherry or pear}\)
  species is the same with possible differences in the mean vectors
  \(\mu_k.\)
\end{enumerate}

\newpage

\subsection{\normalsize{\textit{Parameter Estimation}}}

Given the data collected, for \(k = \text{cherry or pear}\), we estimate
the unknown parameters \(\mu_k\) and \(\Sigma\) as \(\hat{\mu}_k\) and
\(\hat{\Sigma}\) by using the method of moments as follows.

\[ \hat{\mu}_k =
\left(
\begin{matrix}
\frac{1}{n_k}\sum_{i=1}^{n_k} x_{ki} \\
\frac{1}{n_k}\sum_{i=1}^{n_k} y_{ki}
\end{matrix}
\right) \quad \text{and} \quad 
\hat{\Sigma} = \frac{1}{2} \left(\hat{\Sigma}_{\text{cherry}} + \hat{\Sigma}_{\text{pear}} \right)\]
where

\begin{align*}
x_{ki} &= \text{ width (mm) of the $i$-th leaf for the $k$-th species} \\
y_{ki} &= \text{ length (mm) of the $i$-th leaf for the $k$-th species} \\
n_{k} &= \text{ number of total leaves gathered for the $k$-th species} \\
\hat{\Sigma}_k &=
\left(\begin{matrix}
\frac{1}{n_k}\sum_{i=1}^{n_k} x_{ki}^2 - \left(\frac{1}{n_k}\sum_{i=1}^{n_k} x_{ki} \right) ^2
&\frac{1}{n_k}\sum_{i=1}^{n_k} x_{ki} y_{ki} - \frac{1}{n_k}\sum_{i=1}^{n_k} x_{ki} \frac{1}{n_k}\sum_{i=1}^{n_k} y_{ki}
\\
\frac{1}{n_k}\sum_{i=1}^{n_k} x_{ki} y_{ki} - \frac{1}{n_k}\sum_{i=1}^{n_k} x_{ki} \frac{1}{n_k}\sum_{i=1}^{n_k} y_{ki}
&\frac{1}{n_k}\sum_{i=1}^{n_k} y_{ki}^2 - \left(\frac{1}{n_k}\sum_{i=1}^{n_k} y_{ki} \right) ^2
\end{matrix}\right)
\end{align*}

Using the formulas above and the sample data, the estimated mean and
covariance matrix are shown below.

\[\hat{\mu}_{\text{cherry}} = \left(\begin{matrix} 36.44  \\ 79.56 \end{matrix} \right)\]
\[\hat{\mu}_{\text{pear}} = \left(\begin{matrix} 41.19  \\ 70 \end{matrix} \right)\]
\[\hat{\Sigma} = \left( \begin{matrix}22.51 &31.28 \\ 31.28  & 120.5\end{matrix}\right) \]
Hence using the estimated parameters, the density formula for a given
\(x\) width and \(y\) length for the \(k=\text{cherry or pear}\) species
is shown below.

\begin{align*}
f_{\text{cherry}}(x,y \mid \hat{\mu}_{\text{cherry}}, \hat{\Sigma}) &= 
\frac{1}{2\pi\sqrt{\left| \left( \begin{matrix}22.51 &31.28 \\ 31.28  & 120.5\end{matrix}\right) \right|}} 
\exp\left[\frac{-1}{2} 
\left(\begin{matrix}
x-36.44 \\ 
y-79.56
\end{matrix}\right)^T 
\left( \begin{matrix}22.51 &31.28 \\ 31.28  & 120.5\end{matrix}\right) ^{-1} 
\left(\begin{matrix}
x-36.44 \\ 
y-79.56 
\end{matrix}\right) \right] \\
f_{\text{pear}}(x,y \mid \hat{\mu}_{\text{pear}}, \hat{\Sigma}) &= 
\frac{1}{2\pi\sqrt{\left| \left( \begin{matrix}22.51 &31.28 \\ 31.28  & 120.5\end{matrix}\right) \right|}} 
\exp\left[\frac{-1}{2} 
\left(\begin{matrix}
x-41.19 \\ 
y-70
\end{matrix}\right)^T 
\left( \begin{matrix}22.51 &31.28 \\ 31.28  & 120.5\end{matrix}\right) ^{-1} 
\left(\begin{matrix}
x-41.19 \\ 
y-70 
\end{matrix}\right) \right]
\end{align*}

\subsection{\normalsize{\textit{Classification Rule}}}

Let
\(n \text{ be the total number of observations in the training data}\),
\(i\) be any integer from 0 to \(n\), and \((x_i, y_i)\) be the \(i\)-th
observation where \(x_i\) is the width measurement and \(y_i\) is the
length measurement. The respective \(\lambda\) values for each
observation \((x_i, y_i)\) is as follows.

\begin{align*}
\lambda_i &= \frac{f_{\text{cherry}}(x_i,y_i|\hat{\mu}_{\text{cherry}}, \hat{\Sigma})}{f_{\text{pear}} (x_i,y_i|\hat{\mu}_{\text{pear}}, \hat{\Sigma})}  \\
&= \frac{\frac{1}{2\pi\sqrt{\left| \left( \begin{matrix}22.51 &31.28 \\ 31.28  & 120.5\end{matrix}\right) \right|}} 
\exp\left[\frac{-1}{2} 
\left(\begin{matrix}
x-36.44 \\ 
y-79.56
\end{matrix}\right)^T 
\left( \begin{matrix}22.51 &31.28 \\ 31.28  & 120.5\end{matrix}\right) ^{-1} 
\left(\begin{matrix}
x-36.44 \\ 
y-79.56 
\end{matrix}\right) \right]}{\frac{1}{2\pi\sqrt{\left| \left( \begin{matrix}22.51 &31.28 \\ 31.28  & 120.5\end{matrix}\right) \right|}} 
\exp\left[\frac{-1}{2} 
\left(\begin{matrix}
x-41.19 \\ 
y-70
\end{matrix}\right)^T 
\left( \begin{matrix}22.51 &31.28 \\ 31.28  & 120.5\end{matrix}\right) ^{-1} 
\left(\begin{matrix}
x-41.19 \\ 
y-70 
\end{matrix}\right) \right]}
\end{align*}

The following classification rule described below is used to determine
whether observation \((x_i, y_i)\) belongs to a certain species. The
lambda values and classifications of all the observations is recorded in
A2 in the appendix.

\begin{itemize}
\tightlist
\item
  if \(\lambda_i > 1\), then observation \((x_i, y_i)\) is a cherry leaf
\item
  if \(\lambda_i < 1\), then observation \((x_i, y_i)\) is a pear leaf
\item
  if \(\lambda_i = 1\), then observation \((x_i, y_i)\) is undetermined
\end{itemize}

\subsection{\normalsize{\textit{Classification Errors}}}

There is a total of 4 misclassifications. This is evident in table 1 as
well as the rey circles and orange triangles in figure 2.

Table 1. Observation Points Where Misclassification Occurs

\begin{table}[H]
\centering
\begin{tabular}{l|l|r|r|l}
\hline
  & species & width (mm) & length (mm) & classification\\
\hline
5 & cherry & 37 & 67 & pear\\
\hline
7 & cherry & 47 & 88 & pear\\
\hline
25 & pear & 40 & 90 & cherry\\
\hline
27 & pear & 32 & 63 & cherry\\
\hline
\end{tabular}
\end{table}

\newpage

Figure 1. Scatter Plot of Observation Data

\includegraphics{report_files/figure-latex/unnamed-chunk-5-1.pdf}

Figure 2. Scatter Plot of First Classification Rule

\includegraphics{report_files/figure-latex/unnamed-chunk-6-1.pdf}

\newpage

\subsection{\normalsize{\textit{New Classification}}}

Using the classification rules outlined above, we are tasked to classify
new leaves with the following meassurements

\begin{table}[H]
\centering
\begin{tabular}{l|r|r}
\hline
id & width (mm) & length (mm)\\
\hline
u & 32 & 82\\
\hline
v & 38 & 52\\
\hline
w & 52 & 76\\
\hline
\end{tabular}
\end{table}

We calculate \(\lambda\) using the formula stated above and get the
following resuts

\begin{table}[H]
\centering
\begin{tabular}{l|r|l}
\hline
id & lamda & classification\\
\hline
u & 139.61 & cherry\\
\hline
v & 0.01 & pear\\
\hline
w & 0.71 & pear\\
\hline
\end{tabular}
\end{table}

\subsection{\normalsize{\textit{Decision Boundary}}}

Using the derivation of the decision boundary found in A4 in the
appendix, the decision boundary for the given in the formula below.

\[y = 2.4x -18.18 \]

Figure 3. Scatter Plot with Decision Boundary

\includegraphics{report_files/figure-latex/unnamed-chunk-10-1.pdf}

Furthermore, I predicted all the possible points in the observation
space to the closest 0.1 mm using the first classification rules and got
the following results in the plot below. From figure 4, it is clear that
the decision boundary is a linear equation that divides the observation
space into 2 distinct regions.

Figure 4. Grid Plot of a Predicted Points in the Observation Space

\newpage

\section{Second Classification}

\subsection{\normalsize{\textit{Assumptions}}}

The assumptions are similar to the first classification. However, the
only difference is that the covariance matrices are no longer equal.
Instead, for each species \(k\) = cherry or pear, the distribution of
the width and length measurements follow a bivariate normal as follows

\[\left(\begin{matrix}X_k \\ Y_k \end{matrix} \right) \sim N_2 \left(\mu_{k}, \Sigma_k \right)\]
where

\begin{align*}
  \Sigma_k &= \left(\begin{matrix} \sigma^2_{kx} &\sigma_{kxy} \\ \sigma_{kxy} &\sigma^2_{ky} \end{matrix} \right) \text{ covariance matrix of the $k$ species}
  \end{align*}

Hence, the density of a leaf given the \(x\) width and \(y\) length
according to the \(k = \text{cherry or pear}\) species is given by

\[f_k(x,y \mid \mu_k, \Sigma) = \frac{1}{2\pi\sqrt{|\Sigma_k|}} \exp\left[\frac{-1}{2}\ \left(\begin{matrix}x-\mu_{kx} \\ y-\mu_{ky} \end{matrix}\right)^T \Sigma^{-1}_k \left(\begin{matrix}x-\mu_{kx} \\ y-\mu_{ky} \end{matrix}\right) \right] \]

where
\[|\Sigma_k| = \text{ determinant of the $k$ species covariance matrix}.\]

\subsection{\normalsize{\textit{Parameter Estimation}}}

For \(k\) = cherry or pear, \(\hat{\mu}_k\) is the same as derived in
the first classification. \(\hat{\Sigma}_{k}\) is no longer the pooled
covariance matrix. For each \(k\) = cherry or pear the estimation of
\(\hat{\Sigma}_k\) is the method of moments covariance matrix. The
derivation of this can be found in the first classification parameter
estimation.

\[\hat{\Sigma}_{\text{cherry}} = \left( \begin{matrix}27.12 &52.07 \\ 52.07  & 162.87\end{matrix}\right) \]

\[\hat{\Sigma}_{\text{pear}} = \left( \begin{matrix}17.9 &10.5 \\ 10.5  & 78.12\end{matrix}\right) \]
Hence using the estimated parameters, the density formula for a given
\(x\) width and \(y\) length for the \(k=\text{cherry or pear}\) species
is shown below.

\begin{align*}
f_{\text{cherry}}(x,y \mid \hat{\mu}_{\text{cherry}}, \hat{\Sigma}_{\text{cherry}}) &= 
\frac{1}{2\pi\sqrt{\left| \left( \begin{matrix}27.12 &52.07 \\ 52.07  & 162.87\end{matrix}\right) \right|}} 
\exp\left[\frac{-1}{2} 
\left(\begin{matrix}
x-36.44 \\ 
y-79.56
\end{matrix}\right)^T 
\left( \begin{matrix}27.12 &52.07 \\ 52.07  & 162.87\end{matrix}\right) ^{-1} 
\left(\begin{matrix}
x-36.44 \\ 
y-79.56 
\end{matrix}\right) \right] \\
f_{\text{pear}}(x,y \mid \hat{\mu}_{\text{pear}}, \hat{\Sigma}_{\text{pear}}) &= 
\frac{1}{2\pi\sqrt{\left| \left( \begin{matrix}17.9 &10.5 \\ 10.5  & 78.12\end{matrix}\right) \right|}} 
\exp\left[\frac{-1}{2} 
\left(\begin{matrix}
x-41.19 \\ 
y-70
\end{matrix}\right)^T 
\left( \begin{matrix}17.9 &10.5 \\ 10.5  & 78.12\end{matrix}\right) ^{-1} 
\left(\begin{matrix}
x-41.19 \\ 
y-70 
\end{matrix}\right) \right]
\end{align*}

\subsection{\normalsize{\textit{Classification Rule}}}

Similar to the first classification rule, we use the same cutoff
\(\lambda\) value and test cases. However, each corresponding
\(\lambda_i\) is now calculated as follows.

\begin{align*}
\lambda_i &= \frac{f_{\text{cherry}}(x_i,y_i|\hat{\mu}_{\text{cherry}}, \hat{\Sigma}_{\text{cherry}})}{f_{\text{pear}} (x_i,y_i|\hat{\mu}_{\text{pear}}, \hat{\Sigma}_{\text{pear}})}  \\
&= \frac{\frac{1}{2\pi\sqrt{\left| \left( \begin{matrix}27.12 &52.07 \\ 52.07  & 162.87\end{matrix}\right) \right|}} 
\exp\left[\frac{-1}{2} 
\left(\begin{matrix}
x-36.44 \\ 
y-79.56
\end{matrix}\right)^T 
\left( \begin{matrix}27.12 &52.07 \\ 52.07  & 162.87\end{matrix}\right) ^{-1} 
\left(\begin{matrix}
x-36.44 \\ 
y-79.56 
\end{matrix}\right) \right]}{\frac{1}{2\pi\sqrt{\left| \left( \begin{matrix}17.9 &10.5 \\ 10.5  & 78.12\end{matrix}\right) \right|}} 
\exp\left[\frac{-1}{2} 
\left(\begin{matrix}
x-41.19 \\ 
y-70
\end{matrix}\right)^T 
\left( \begin{matrix}17.9 &10.5 \\ 10.5  & 78.12\end{matrix}\right) ^{-1} 
\left(\begin{matrix}
x-41.19 \\ 
y-70 
\end{matrix}\right) \right]}
\end{align*}

\subsection{\normalsize{\textit{Decision Boundary}}}

Using the derivation in A5 in the appendix, the decision boundary for
the given classification is the function below

\[y = \frac{1}{2*A} -{B} - \sqrt{B^2 - 4AC}\] where

\[
A = 0.002 \\
B= -0.04473 x + 0.96849 \\
C = 0.03481 x^2 + 1.75362 x + -73.25789
\]

\newpage

\section{Conclusion}

\newpage

\section{References}

\begin{itemize}
\item
  R Core Team (2020). R: A language and environment for statistical
  computing. R Foundation for Statistical Computing, Vienna, Austria.
  URL \url{https://www.R-project.org./}
\item
  H. Wickham (2016). ggplot2: Elegant Graphics for Data Analysis.
  Springer-Verlag New York, 2016.
\item
  Baptiste Auguie (2017). gridExtra: Miscellaneous Functions for
  ``Grid'' Graphics. R package version 2.3.
  \url{https://CRAN.R-project.org/package=gridExtra}
\item
  Alan Genz, Frank Bretz, Tetsuhisa Miwa, Xuefei Mi, Friedrich Leisch,
  Fabian Scheipl, Torsten Hothorn (2020). mvtnorm: Multivariate Normal
  and t Distributions. R package version 1.1-1. URL
  \url{http://CRAN.R-project.org/package=mvtnorm}
\item
  Alan Genz, Frank Bretz (2009), Computation of Multivariate Normal and
  t Probabilities. Lecture Notes in Statistics, Vol. 195.,
  Springer-Verlag, Heidelberg. ISBN 978-3-642-01688-2
\end{itemize}

\newpage

\section{Appendix}

\textbf{A1. Data Table with Density Calculations}

\begin{verbatim}
##    species width length
## 1   cherry    30     56
## 2   cherry    32     59
## 3   cherry    31     68
## 4   cherry    41     90
## 5   cherry    37     67
## 6   cherry    45    105
## 7   cherry    47     88
## 8   cherry    31     73
## 9   cherry    32     73
## 10  cherry    33     80
## 11  cherry    36     82
## 12  cherry    40     89
## 13  cherry    42     97
## 14  cherry    37     80
## 15  cherry    31     83
## 16  cherry    38     83
## 17    pear    44     58
## 18    pear    41     68
## 19    pear    38     61
## 20    pear    40     62
## 21    pear    42     78
## 22    pear    40     66
## 23    pear    40     63
## 24    pear    47     65
## 25    pear    40     90
## 26    pear    42     76
## 27    pear    32     63
## 28    pear    45     84
## 29    pear    42     79
## 30    pear    36     66
## 31    pear    39     67
## 32    pear    51     74
\end{verbatim}

\newpage

\textbf{A2. Densities, Lambda Calculations and First Rule Classification}

\begin{verbatim}
##    species f.cherry.1 f.pear.1     l.1    c.1
## 1   cherry    3.8e-04  2.3e-04 1.6e+00 cherry
## 2   cherry    6.4e-04  5.7e-04 1.1e+00 cherry
## 3   cherry    1.8e-03  1.5e-04 1.2e+01 cherry
## 4   cherry    2.2e-03  2.7e-04 8.1e+00 cherry
## 5   cherry    1.2e-03  2.5e-03 4.9e-01   pear
## 6   cherry    2.3e-04  9.0e-06 2.5e+01 cherry
## 7   cherry    2.5e-04  9.5e-04 2.6e-01   pear
## 8   cherry    2.0e-03  5.6e-05 3.5e+01 cherry
## 9   cherry    2.5e-03  1.2e-04 2.1e+01 cherry
## 10  cherry    2.5e-03  4.4e-05 5.6e+01 cherry
## 11  cherry    3.6e-03  1.9e-04 1.9e+01 cherry
## 12  cherry    2.5e-03  2.3e-04 1.1e+01 cherry
## 13  cherry    1.0e-03  4.9e-05 2.1e+01 cherry
## 14  cherry    3.8e-03  5.1e-04 7.4e+00 cherry
## 15  cherry    9.0e-04  3.2e-06 2.8e+02 cherry
## 16  cherry    3.6e-03  4.2e-04 8.4e+00 cherry
## 17    pear    1.4e-06  6.2e-04 2.2e-03   pear
## 18    pear    3.0e-04  3.7e-03 8.0e-02   pear
## 19    pear    2.2e-04  2.7e-03 8.3e-02   pear
## 20    pear    1.1e-04  2.9e-03 3.8e-02   pear
## 21    pear    1.1e-03  2.8e-03 4.0e-01   pear
## 22    pear    3.1e-04  3.6e-03 8.7e-02   pear
## 23    pear    1.4e-04  3.1e-03 4.6e-02   pear
## 24    pear    1.2e-06  5.9e-04 2.1e-03   pear
## 25    pear    2.4e-03  1.8e-04 1.3e+01 cherry
## 26    pear    8.4e-04  3.2e-03 2.6e-01   pear
## 27    pear    1.2e-03  4.7e-04 2.6e+00 cherry
## 28    pear    5.2e-04  1.7e-03 3.1e-01   pear
## 29    pear    1.2e-03  2.5e-03 4.9e-01   pear
## 30    pear    1.3e-03  2.0e-03 6.5e-01   pear
## 31    pear    6.1e-04  3.4e-03 1.8e-01   pear
## 32    pear    4.6e-07  2.5e-04 1.9e-03   pear
\end{verbatim}

where

\begin{align*}
\texttt{f.k.1} &= \text{$f_k(x,y|\hat{\mu}_{k}, \hat{\Sigma})$ of the $k$ species} \\ 
\texttt{l.1} &= \text{$\lambda = \frac{f_{\text{cherry}}(x,y|\hat{\mu}_{\text{cherry}}, \hat{\Sigma})}{f_{\text{pear}} (x,y|\hat{\mu}_{\text{pear}}, \hat{\Sigma})}$ calculation} \\
\texttt{c.1} &= \text{"cherry" or "pear" depending on the first classification rule} \\
\end{align*}

\newpage

\textbf{A3. Densities, Lambda Calculations and Second Rule Classification}

\begin{verbatim}
##    species f.cherry.2 f.pear.2     l.2    c.2
## 1   cherry    6.6e-04  9.2e-05 7.2e+00 cherry
## 2   cherry    8.5e-04  3.4e-04 2.5e+00 cherry
## 3   cherry    2.2e-03  2.2e-04 1.0e+01 cherry
## 4   cherry    2.6e-03  2.7e-04 9.6e+00 cherry
## 5   cherry    8.7e-04  2.7e-03 3.2e-01   pear
## 6   cherry    5.2e-04  1.7e-06 3.1e+02 cherry
## 7   cherry    1.6e-04  3.9e-04 4.1e-01   pear
## 8   cherry    2.0e-03  1.4e-04 1.4e+01 cherry
## 9   cherry    2.6e-03  2.6e-04 1.0e+01 cherry
## 10  cherry    2.1e-03  1.5e-04 1.4e+01 cherry
## 11  cherry    3.5e-03  4.3e-04 8.1e+00 cherry
## 12  cherry    2.9e-03  2.9e-04 1.0e+01 cherry
## 13  cherry    1.5e-03  3.3e-05 4.6e+01 cherry
## 14  cherry    3.8e-03  9.2e-04 4.1e+00 cherry
## 15  cherry    4.8e-04  2.0e-05 2.4e+01 cherry
## 16  cherry    3.7e-03  7.2e-04 5.1e+00 cherry
## 17    pear    4.3e-08  9.7e-04 4.4e-05   pear
## 18    pear    9.9e-05  4.3e-03 2.3e-02   pear
## 19    pear    9.2e-05  2.3e-03 3.9e-02   pear
## 20    pear    2.7e-05  2.9e-03 9.1e-03   pear
## 21    pear    6.6e-04  2.9e-03 2.3e-01   pear
## 22    pear    1.1e-04  4.0e-03 2.8e-02   pear
## 23    pear    3.9e-05  3.2e-03 1.2e-02   pear
## 24    pear    3.2e-08  1.1e-03 3.0e-05   pear
## 25    pear    2.8e-03  2.2e-04 1.3e+01 cherry
## 26    pear    4.3e-04  3.5e-03 1.2e-01   pear
## 27    pear    1.6e-03  4.1e-04 3.9e+00 cherry
## 28    pear    3.2e-04  1.1e-03 2.8e-01   pear
## 29    pear    8.0e-04  2.6e-03 3.0e-01   pear
## 30    pear    1.1e-03  2.1e-03 5.1e-01   pear
## 31    pear    3.0e-04  3.8e-03 7.9e-02   pear
## 32    pear    1.0e-08  2.9e-04 3.5e-05   pear
\end{verbatim}

where

\begin{align*}
\texttt{f.k.2} &= \text{$f_k(x,y|\hat{\mu}_{k}, \hat{\Sigma}_{k})$ of the $k$ species} \\
\texttt{l.2} &= \text{$\lambda = \frac{f_{\text{cherry}}(x,y|\hat{\mu}_{\text{cherry}}, \hat{\Sigma}_{\text{cherry}})}{f_{\text{pear}} (x,y|\hat{\mu}_{\text{pear}}, \hat{\Sigma}_{\text{pear}})}$ calculation} \\
\texttt{c.2} &= \text{"cherry" or "pear" depending on the second classification rule} \\
\end{align*}

\newpage

\textbf{A4. Calculating Decision Boundary for First Classification}

The decision boundary is the equation \(y = f(x)\) such that
\(\lambda=1\). In order to have cleaner derivation, let
\(a = \left( \begin{matrix}x \\y \end{matrix}\right)\). We calculate the
decision boundary by the following

\begin{align*}
\lambda &= \frac{f_{\text{cherry}}(x_i,y_i|\hat{\mu}_{\text{cherry}}, \hat{\Sigma})}{f_{\text{pear}} (x_i,y_i|\hat{\mu}_{\text{pear}}, \hat{\Sigma})} \\
&= \frac{\frac{1}{2\pi\sqrt{|\hat{\Sigma}_{\text{}}|}} \exp\left\{-\frac12(a - \hat{\mu}_{cherry})^T \hat{\Sigma}_{\text{}}^{-1} (a - \hat{\mu}_{cherry}) \right \}}
{\frac{1}{2\pi\sqrt{|\hat{\Sigma}_{\text{}}|}} \exp\left\{-\frac12 (a - \hat{\mu}_{pear})^T \hat{\Sigma}_{\text{}}^{-1} (a - \hat{\mu}_{pear}) \right \}} \\
&= \exp \left\{ 
-\frac12 
\left(
(a - \hat{\mu}_{cherry})^T 
\hat{\Sigma}_{\text{}}^{-1}
(a - \hat{\mu}_{cherry}) - 
(a - \hat{\mu}_{pear})^T 
\hat{\Sigma}_{\text{}}^{-1} 
(a - \hat{\mu}_{pear})
\right)
\right\} 
\end{align*}

Since, the cutoff value is at \(\lambda=1\), we set the \(\lambda\)
equal to 1 and simplify.

\begin{align*}
1 &= \exp \left\{ 
-\frac12 
\left(
(a - \hat{\mu}_{cherry})^T 
\hat{\Sigma}_{\text{}}^{-1}
(a - \hat{\mu}_{cherry}) - 
(a - \hat{\mu}_{pear})^T 
\hat{\Sigma}_{\text{}}^{-1} 
(a - \hat{\mu}_{pear})
\right)
\right\} 
\\ 
\implies \log (1) 
&= log( \exp \left\{ 
-\frac12 
\left(
(a - \hat{\mu}_{cherry})^T 
\hat{\Sigma}_{\text{}}^{-1}
(a - \hat{\mu}_{cherry}) - 
(a - \hat{\mu}_{pear})^T 
\hat{\Sigma}_{\text{}}^{-1} 
(a - \hat{\mu}_{pear})
\right)
\right\}) 
\\
\implies 0
&=-\frac12 
\left(
(a - \hat{\mu}_{cherry})^T 
\hat{\Sigma}_{\text{}}^{-1}
(a - \hat{\mu}_{cherry}) - 
(a - \hat{\mu}_{pear})^T 
\hat{\Sigma}_{\text{}}^{-1} 
(a - \hat{\mu}_{pear})
\right) 
\end{align*}

Since \(\log (1) = 0\), we get the following equality

\begin{align*}
&(a - \hat{\mu}_{cherry})^T 
\hat{\Sigma}_{\text{}}
(a - \hat{\mu}_{cherry}) = (a - \hat{\mu}_{pear})^T 
\hat{\Sigma}_{\text{pear}}^{-1} 
(a - \hat{\mu}_{pear}) 
\\
\implies 0 &= (a - \hat{\mu}_{cherry})^T 
\hat{\Sigma}_{\text{}}^{-1}
(a - \hat{\mu}_{cherry}) - (a - \hat{\mu}_{pear})^T 
\hat{\Sigma}_{\text{pear}}^{-1} 
(a - \hat{\mu}_{pear}) 
\\
0 &=a^T\hat{\Sigma}_{\text{}}^{-1}a 
+ 2a^T
\hat{\Sigma}^{-1}
\hat{\mu}_{cherry}
- \hat{\mu}_{cherry}^T
\hat{\Sigma}^{-1}
\hat{\mu}_{cherry}
- a^T
\hat{\Sigma}_{\text{}}^{-1}
a 
- 2a^T
\hat{\Sigma}^{-1}
\hat{\mu}_{pear} 
+ \hat{\mu}_{pear}^T
\hat{\Sigma}^{-1}
\hat{\mu}_{pear}
\\
0 &= a^T 
\underbrace{
2\hat{\Sigma}^{-1}}_{A} 
\underbrace{
(\hat{\mu}_{cherry} - \hat{\mu}_{pear})}_{B}
- \underbrace{(\hat{\mu}_{cherry}^T
\hat{\Sigma}^{-1}
\hat{\mu}_{cherry}
- \hat{\mu}_{pear}^T
\hat{\Sigma}
\hat{\mu}_{pear})}_{C} 
\end{align*}

Since, \(A\) is a \(2\times 2\) matrix, \(B\) is \(2\times 1\) and \(C\)
is a constant then we can express the equality in terms of \(x\) and
\(y\) then solve for \(y\).

\begin{align*}
0 &=
a^TAB - C \\
&= 
\left(
\begin{matrix}
x &y
\end{matrix}
\right)
\left(
\begin{matrix}
m &n \\ n &o
\end{matrix}
\right)
\left(
\begin{matrix}
p \\q
\end{matrix}
\right) - c \\
&= q(mx + oy) + r(nx + py) - c \\
\implies y &= -\frac{qm+rn}{qo+rp}x + \frac{c}{qo+rp} \\
y&= f(x) \text{ is the decision boundary where $\lambda=1$ as desired.}
\end{align*}

\end{document}
